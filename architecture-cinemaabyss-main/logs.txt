
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COMMAND ‚îÇ                                                                               ARGS                                                                                ‚îÇ PROFILE  ‚îÇ       USER       ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start   ‚îÇ                                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 16 Oct 25 13:42 +07 ‚îÇ 16 Oct 25 13:49 +07 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                                                                                                                                    ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 16 Oct 25 13:50 +07 ‚îÇ 16 Oct 25 13:53 +07 ‚îÇ
‚îÇ tunnel  ‚îÇ                                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 16 Oct 25 14:09 +07 ‚îÇ                     ‚îÇ
‚îÇ delete  ‚îÇ                                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 26 Oct 25 22:10 +07 ‚îÇ 26 Oct 25 22:10 +07 ‚îÇ
‚îÇ start   ‚îÇ --driver=docker                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 26 Oct 25 22:10 +07 ‚îÇ 26 Oct 25 22:13 +07 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                                                                                                                                    ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 26 Oct 25 23:58 +07 ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                                                                                                                                    ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:12 +07 ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ disable ingress                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:17 +07 ‚îÇ 27 Oct 25 00:17 +07 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress --force                                                                                                                                            ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:17 +07 ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                                                                                                                                    ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:27 +07 ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ disable ingress                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:35 +07 ‚îÇ 27 Oct 25 00:35 +07 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                                                                                                                                    ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:35 +07 ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ disable ingress                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:42 +07 ‚îÇ 27 Oct 25 00:42 +07 ‚îÇ
‚îÇ ssh     ‚îÇ curl -I https://registry.k8s.io                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:48 +07 ‚îÇ 27 Oct 25 00:48 +07 ‚îÇ
‚îÇ addons  ‚îÇ disable ingress                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:50 +07 ‚îÇ 27 Oct 25 00:50 +07 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress --images=IngressController=k8s.gcr.io/ingress-nginx/controller:v1.3.0,IngressAdmissionWebhook=k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.3.0 ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 00:51 +07 ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ disable ingress                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 01:01 +07 ‚îÇ 27 Oct 25 01:01 +07 ‚îÇ
‚îÇ addons  ‚îÇ disable ingress                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 01:03 +07 ‚îÇ 27 Oct 25 01:03 +07 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                                                                                                                                    ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 01:04 +07 ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ disable ingress                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 01:10 +07 ‚îÇ 27 Oct 25 01:10 +07 ‚îÇ
‚îÇ addons  ‚îÇ enable kong                                                                                                                                                       ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 01:12 +07 ‚îÇ 27 Oct 25 01:12 +07 ‚îÇ
‚îÇ ip      ‚îÇ                                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 01:14 +07 ‚îÇ 27 Oct 25 01:14 +07 ‚îÇ
‚îÇ ip      ‚îÇ                                                                                                                                                                   ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 01:14 +07 ‚îÇ 27 Oct 25 01:14 +07 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                                                                                                                                    ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 01:16 +07 ‚îÇ                     ‚îÇ
‚îÇ addons  ‚îÇ enable ingress                                                                                                                                                    ‚îÇ minikube ‚îÇ SHEVCHUKVV\nika1 ‚îÇ v1.37.0 ‚îÇ 27 Oct 25 12:39 +07 ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/10/26 22:10:43
Running on machine: ShevchukVV
Binary: Built with gc go1.24.6 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1026 22:10:43.895106   41760 out.go:360] Setting OutFile to fd 508 ...
I1026 22:10:43.916443   41760 out.go:413] isatty.IsTerminal(508) = true
I1026 22:10:43.916443   41760 out.go:374] Setting ErrFile to fd 508...
I1026 22:10:43.916443   41760 out.go:413] isatty.IsTerminal(508) = true
I1026 22:10:43.944696   41760 out.go:368] Setting JSON to false
I1026 22:10:43.946868   41760 start.go:130] hostinfo: {"hostname":"ShevchukVV","uptime":374008,"bootTime":1761117435,"procs":337,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.6899 Build 26100.6899","kernelVersion":"10.0.26100.6899 Build 26100.6899","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"6dc98ee4-5bde-4780-99b0-4021a31195e1"}
W1026 22:10:43.946868   41760 start.go:138] gopshost.Virtualization returned error: not implemented yet
I1026 22:10:43.949611   41760 out.go:179] üòÑ  minikube v1.37.0 on Microsoft Windows 11 Home Single Language 10.0.26100.6899 Build 26100.6899
I1026 22:10:43.951170   41760 notify.go:220] Checking for updates...
I1026 22:10:43.954418   41760 driver.go:421] Setting default libvirt URI to qemu:///system
I1026 22:10:44.062949   41760 docker.go:123] docker version: linux-27.4.0:Docker Desktop 4.37.1 (178610)
I1026 22:10:44.069368   41760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1026 22:10:44.885772   41760 info.go:266] docker info: {ID:de6a7e5f-4c83-4a12-bf5c-ddc85b47df70 Containers:27 ContainersRunning:26 ContainersPaused:0 ContainersStopped:1 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:249 OomKillDisable:false NGoroutines:295 SystemTime:2025-10-26 15:10:44.864508693 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:19 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7968342016 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.5.1] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.31.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.0] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.1]] Warnings:<nil>}}
I1026 22:10:44.887382   41760 out.go:179] ‚ú®  Using the docker driver based on user configuration
I1026 22:10:44.888450   41760 start.go:304] selected driver: docker
I1026 22:10:44.888450   41760 start.go:918] validating driver "docker" against <nil>
I1026 22:10:44.888450   41760 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1026 22:10:44.899846   41760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1026 22:10:45.166572   41760 info.go:266] docker info: {ID:de6a7e5f-4c83-4a12-bf5c-ddc85b47df70 Containers:27 ContainersRunning:26 ContainersPaused:0 ContainersStopped:1 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:249 OomKillDisable:false NGoroutines:295 SystemTime:2025-10-26 15:10:45.146375456 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:19 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7968342016 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.5.1] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.31.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.0] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.1]] Warnings:<nil>}}
I1026 22:10:45.166572   41760 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I1026 22:10:45.223469   41760 start_flags.go:410] Using suggested 3900MB memory alloc based on sys=15693MB, container=7599MB
I1026 22:10:45.223469   41760 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I1026 22:10:45.225080   41760 out.go:179] üìå  Using Docker Desktop driver with root privileges
I1026 22:10:45.226644   41760 cni.go:84] Creating CNI manager for ""
I1026 22:10:45.227170   41760 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1026 22:10:45.227170   41760 start_flags.go:336] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1026 22:10:45.228209   41760 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1026 22:10:45.230031   41760 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1026 22:10:45.231079   41760 cache.go:123] Beginning downloading kic base image for docker with docker
I1026 22:10:45.231615   41760 out.go:179] üöú  Pulling base image v0.0.48 ...
I1026 22:10:45.233181   41760 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1026 22:10:45.233181   41760 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1026 22:10:45.234746   41760 preload.go:146] Found local preload: C:\Users\nika1\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1026 22:10:45.234746   41760 cache.go:58] Caching tarball of preloaded images
I1026 22:10:45.235281   41760 preload.go:172] Found C:\Users\nika1\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1026 22:10:45.235281   41760 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1026 22:10:45.236409   41760 profile.go:143] Saving config to C:\Users\nika1\.minikube\profiles\minikube\config.json ...
I1026 22:10:45.236927   41760 lock.go:35] WriteFile acquiring C:\Users\nika1\.minikube\profiles\minikube\config.json: {Name:mkc8de7b1ed39b0ad5b605f8f1e30d2980e487fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:10:45.310173   41760 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1026 22:10:45.310704   41760 localpath.go:148] windows sanitize: C:\Users\nika1\.minikube\cache\kic\amd64\kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\nika1\.minikube\cache\kic\amd64\kicbase_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1026 22:10:45.311232   41760 localpath.go:148] windows sanitize: C:\Users\nika1\.minikube\cache\kic\amd64\kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\nika1\.minikube\cache\kic\amd64\kicbase_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1026 22:10:45.311232   41760 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1026 22:10:45.312800   41760 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory, skipping pull
I1026 22:10:45.312800   41760 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in cache, skipping pull
I1026 22:10:45.312800   41760 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1026 22:10:45.313383   41760 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1026 22:10:45.313383   41760 localpath.go:148] windows sanitize: C:\Users\nika1\.minikube\cache\kic\amd64\kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar -> C:\Users\nika1\.minikube\cache\kic\amd64\kicbase_v0.0.48@sha256_7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1.tar
I1026 22:12:19.577422   41760 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1026 22:12:19.578514   41760 cache.go:232] Successfully downloaded all kic artifacts
I1026 22:12:19.580149   41760 start.go:360] acquireMachinesLock for minikube: {Name:mk941a0a9ee4650ba814769e41d656cb559b8aa8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1026 22:12:19.580149   41760 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1026 22:12:19.580689   41760 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1026 22:12:19.581227   41760 start.go:125] createHost starting for "" (driver="docker")
I1026 22:12:19.583902   41760 out.go:252] üî•  Creating docker container (CPUs=2, Memory=3900MB) ...
I1026 22:12:19.585556   41760 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1026 22:12:19.585556   41760 client.go:168] LocalClient.Create starting
I1026 22:12:19.587687   41760 main.go:141] libmachine: Reading certificate data from C:\Users\nika1\.minikube\certs\ca.pem
I1026 22:12:19.599238   41760 main.go:141] libmachine: Decoding PEM data...
I1026 22:12:19.600185   41760 main.go:141] libmachine: Parsing certificate...
I1026 22:12:19.602265   41760 main.go:141] libmachine: Reading certificate data from C:\Users\nika1\.minikube\certs\cert.pem
I1026 22:12:19.612049   41760 main.go:141] libmachine: Decoding PEM data...
I1026 22:12:19.612049   41760 main.go:141] libmachine: Parsing certificate...
I1026 22:12:19.620812   41760 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1026 22:12:19.685286   41760 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1026 22:12:19.690472   41760 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1026 22:12:19.690472   41760 cli_runner.go:164] Run: docker network inspect minikube
W1026 22:12:19.755506   41760 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1026 22:12:19.755506   41760 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1026 22:12:19.755506   41760 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1026 22:12:19.760370   41760 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1026 22:12:19.852888   41760 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001e792f0}
I1026 22:12:19.853416   41760 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1026 22:12:19.858320   41760 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1026 22:12:20.026013   41760 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1026 22:12:20.026563   41760 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1026 22:12:20.040423   41760 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1026 22:12:20.114387   41760 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1026 22:12:20.192803   41760 oci.go:103] Successfully created a docker volume minikube
I1026 22:12:20.198177   41760 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I1026 22:12:21.851140   41760 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib: (1.6529628s)
I1026 22:12:21.851140   41760 oci.go:107] Successfully prepared a docker volume minikube
I1026 22:12:21.851140   41760 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1026 22:12:21.851680   41760 kic.go:194] Starting extracting preloaded images to volume ...
I1026 22:12:21.865208   41760 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\nika1\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I1026 22:12:39.812840   41760 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\nika1\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (17.947632s)
I1026 22:12:39.812840   41760 kic.go:203] duration metric: took 17.9616999s to extract preloaded images to volume ...
I1026 22:12:39.821554   41760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1026 22:12:40.118544   41760 info.go:266] docker info: {ID:de6a7e5f-4c83-4a12-bf5c-ddc85b47df70 Containers:28 ContainersRunning:26 ContainersPaused:0 ContainersStopped:2 Images:25 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:240 OomKillDisable:false NGoroutines:277 SystemTime:2025-10-26 15:12:40.098095981 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7968342016 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.4.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.5.1] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.19.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.31.0-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.0] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.1]] Warnings:<nil>}}
I1026 22:12:40.125535   41760 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1026 22:12:40.415452   41760 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb --memory-swap=3900mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I1026 22:12:41.199394   41760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1026 22:12:41.276733   41760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1026 22:12:41.352359   41760 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1026 22:12:41.539172   41760 oci.go:144] the created container "minikube" has a running status.
I1026 22:12:41.539802   41760 kic.go:225] Creating ssh key for kic: C:\Users\nika1\.minikube\machines\minikube\id_rsa...
I1026 22:12:41.948904   41760 kic_runner.go:191] docker (temp): C:\Users\nika1\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1026 22:12:42.113182   41760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1026 22:12:42.214288   41760 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1026 22:12:42.214288   41760 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1026 22:12:42.355240   41760 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\nika1\.minikube\machines\minikube\id_rsa...
I1026 22:12:43.043668   41760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1026 22:12:43.108034   41760 machine.go:93] provisionDockerMachine start ...
I1026 22:12:43.114000   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:43.182160   41760 main.go:141] libmachine: Using SSH client type: native
I1026 22:12:43.195516   41760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13617c0] 0x1364300 <nil>  [] 0s} 127.0.0.1 53715 <nil> <nil>}
I1026 22:12:43.195516   41760 main.go:141] libmachine: About to run SSH command:
hostname
I1026 22:12:43.371170   41760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1026 22:12:43.372800   41760 ubuntu.go:182] provisioning hostname "minikube"
I1026 22:12:43.378949   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:43.442934   41760 main.go:141] libmachine: Using SSH client type: native
I1026 22:12:43.443463   41760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13617c0] 0x1364300 <nil>  [] 0s} 127.0.0.1 53715 <nil> <nil>}
I1026 22:12:43.443463   41760 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1026 22:12:43.657414   41760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1026 22:12:43.665562   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:43.730081   41760 main.go:141] libmachine: Using SSH client type: native
I1026 22:12:43.730634   41760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13617c0] 0x1364300 <nil>  [] 0s} 127.0.0.1 53715 <nil> <nil>}
I1026 22:12:43.730634   41760 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1026 22:12:43.895495   41760 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1026 22:12:43.908914   41760 ubuntu.go:188] set auth options {CertDir:C:\Users\nika1\.minikube CaCertPath:C:\Users\nika1\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\nika1\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\nika1\.minikube\machines\server.pem ServerKeyPath:C:\Users\nika1\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\nika1\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\nika1\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\nika1\.minikube}
I1026 22:12:43.908914   41760 ubuntu.go:190] setting up certificates
I1026 22:12:43.908914   41760 provision.go:84] configureAuth start
I1026 22:12:43.913602   41760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1026 22:12:43.977474   41760 provision.go:143] copyHostCerts
I1026 22:12:43.987703   41760 exec_runner.go:144] found C:\Users\nika1\.minikube/cert.pem, removing ...
I1026 22:12:43.987703   41760 exec_runner.go:203] rm: C:\Users\nika1\.minikube\cert.pem
I1026 22:12:43.988225   41760 exec_runner.go:151] cp: C:\Users\nika1\.minikube\certs\cert.pem --> C:\Users\nika1\.minikube/cert.pem (1119 bytes)
I1026 22:12:43.999581   41760 exec_runner.go:144] found C:\Users\nika1\.minikube/key.pem, removing ...
I1026 22:12:43.999581   41760 exec_runner.go:203] rm: C:\Users\nika1\.minikube\key.pem
I1026 22:12:43.999581   41760 exec_runner.go:151] cp: C:\Users\nika1\.minikube\certs\key.pem --> C:\Users\nika1\.minikube/key.pem (1679 bytes)
I1026 22:12:44.000640   41760 exec_runner.go:144] found C:\Users\nika1\.minikube/ca.pem, removing ...
I1026 22:12:44.000640   41760 exec_runner.go:203] rm: C:\Users\nika1\.minikube\ca.pem
I1026 22:12:44.001156   41760 exec_runner.go:151] cp: C:\Users\nika1\.minikube\certs\ca.pem --> C:\Users\nika1\.minikube/ca.pem (1074 bytes)
I1026 22:12:44.002810   41760 provision.go:117] generating server cert: C:\Users\nika1\.minikube\machines\server.pem ca-key=C:\Users\nika1\.minikube\certs\ca.pem private-key=C:\Users\nika1\.minikube\certs\ca-key.pem org=nika1.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1026 22:12:44.174913   41760 provision.go:177] copyRemoteCerts
I1026 22:12:44.177896   41760 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1026 22:12:44.183678   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:44.255566   41760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53715 SSHKeyPath:C:\Users\nika1\.minikube\machines\minikube\id_rsa Username:docker}
I1026 22:12:44.376734   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1026 22:12:44.422550   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1026 22:12:44.474237   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1026 22:12:44.521871   41760 provision.go:87] duration metric: took 611.9ms to configureAuth
I1026 22:12:44.521871   41760 ubuntu.go:206] setting minikube options for container-runtime
I1026 22:12:44.521871   41760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1026 22:12:44.526716   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:44.591784   41760 main.go:141] libmachine: Using SSH client type: native
I1026 22:12:44.592305   41760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13617c0] 0x1364300 <nil>  [] 0s} 127.0.0.1 53715 <nil> <nil>}
I1026 22:12:44.592305   41760 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1026 22:12:44.760607   41760 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1026 22:12:44.760607   41760 ubuntu.go:71] root file system type: overlay
I1026 22:12:44.761690   41760 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1026 22:12:44.766417   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:44.830000   41760 main.go:141] libmachine: Using SSH client type: native
I1026 22:12:44.830520   41760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13617c0] 0x1364300 <nil>  [] 0s} 127.0.0.1 53715 <nil> <nil>}
I1026 22:12:44.830520   41760 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1026 22:12:45.015351   41760 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1026 22:12:45.033282   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:45.114827   41760 main.go:141] libmachine: Using SSH client type: native
I1026 22:12:45.114827   41760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x13617c0] 0x1364300 <nil>  [] 0s} 127.0.0.1 53715 <nil> <nil>}
I1026 22:12:45.114827   41760 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1026 22:12:46.438252   41760 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-10-26 15:12:45.008752078 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1026 22:12:46.438252   41760 machine.go:96] duration metric: took 3.3302174s to provisionDockerMachine
I1026 22:12:46.438252   41760 client.go:171] duration metric: took 26.852696s to LocalClient.Create
I1026 22:12:46.438252   41760 start.go:167] duration metric: took 26.852696s to libmachine.API.Create "minikube"
I1026 22:12:46.438252   41760 start.go:293] postStartSetup for "minikube" (driver="docker")
I1026 22:12:46.438252   41760 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1026 22:12:46.440454   41760 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1026 22:12:46.446476   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:46.517864   41760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53715 SSHKeyPath:C:\Users\nika1\.minikube\machines\minikube\id_rsa Username:docker}
I1026 22:12:46.645441   41760 ssh_runner.go:195] Run: cat /etc/os-release
I1026 22:12:46.653543   41760 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1026 22:12:46.653543   41760 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1026 22:12:46.653543   41760 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1026 22:12:46.653543   41760 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1026 22:12:46.653543   41760 filesync.go:126] Scanning C:\Users\nika1\.minikube\addons for local assets ...
I1026 22:12:46.654123   41760 filesync.go:126] Scanning C:\Users\nika1\.minikube\files for local assets ...
I1026 22:12:46.654123   41760 start.go:296] duration metric: took 215.8708ms for postStartSetup
I1026 22:12:46.660437   41760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1026 22:12:46.724552   41760 profile.go:143] Saving config to C:\Users\nika1\.minikube\profiles\minikube\config.json ...
I1026 22:12:46.727770   41760 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1026 22:12:46.733085   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:46.794198   41760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53715 SSHKeyPath:C:\Users\nika1\.minikube\machines\minikube\id_rsa Username:docker}
I1026 22:12:46.905375   41760 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1026 22:12:46.914558   41760 start.go:128] duration metric: took 27.3333303s to createHost
I1026 22:12:46.914558   41760 start.go:83] releasing machines lock for "minikube", held for 27.3344082s
I1026 22:12:46.919026   41760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1026 22:12:46.981135   41760 ssh_runner.go:195] Run: cat /version.json
I1026 22:12:46.985858   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:46.993858   41760 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1026 22:12:47.001153   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:12:47.052203   41760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53715 SSHKeyPath:C:\Users\nika1\.minikube\machines\minikube\id_rsa Username:docker}
I1026 22:12:47.064629   41760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53715 SSHKeyPath:C:\Users\nika1\.minikube\machines\minikube\id_rsa Username:docker}
W1026 22:12:47.177094   41760 start.go:868] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1026 22:12:47.178701   41760 ssh_runner.go:195] Run: systemctl --version
I1026 22:12:47.191857   41760 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1026 22:12:47.205208   41760 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1026 22:12:47.224231   41760 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1026 22:12:47.226847   41760 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1026 22:12:47.295963   41760 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I1026 22:12:47.295963   41760 start.go:495] detecting cgroup driver to use...
I1026 22:12:47.296470   41760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1026 22:12:47.298067   41760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1026 22:12:47.334131   41760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1026 22:12:47.355674   41760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1026 22:12:47.377807   41760 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1026 22:12:47.379462   41760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1026 22:12:47.401153   41760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1026 22:12:47.422741   41760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1026 22:12:47.443813   41760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1026 22:12:47.464271   41760 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1026 22:12:47.484700   41760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1026 22:12:47.506043   41760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1026 22:12:47.527078   41760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1026 22:12:47.551020   41760 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1026 22:12:47.570004   41760 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1026 22:12:47.590109   41760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1026 22:12:47.696643   41760 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1026 22:12:47.857517   41760 start.go:495] detecting cgroup driver to use...
I1026 22:12:47.857517   41760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1026 22:12:47.860878   41760 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1026 22:12:47.887093   41760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1026 22:12:47.911890   41760 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1026 22:12:47.969389   41760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1026 22:12:47.994175   41760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1026 22:12:48.018230   41760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1026 22:12:48.052207   41760 ssh_runner.go:195] Run: which cri-dockerd
I1026 22:12:48.062382   41760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1026 22:12:48.081444   41760 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1026 22:12:48.119107   41760 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1026 22:12:48.229144   41760 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1026 22:12:48.338493   41760 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I1026 22:12:48.338493   41760 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
W1026 22:12:48.352704   41760 out.go:285] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1026 22:12:48.353243   41760 out.go:285] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1026 22:12:48.374501   41760 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1026 22:12:48.400773   41760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1026 22:12:48.510177   41760 ssh_runner.go:195] Run: sudo systemctl restart docker
I1026 22:12:49.356418   41760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1026 22:12:49.380977   41760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1026 22:12:49.407071   41760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1026 22:12:49.432764   41760 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1026 22:12:49.554342   41760 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1026 22:12:49.663473   41760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1026 22:12:49.757622   41760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1026 22:12:49.816850   41760 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1026 22:12:49.843114   41760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1026 22:12:49.955423   41760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1026 22:12:50.220208   41760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1026 22:12:50.242969   41760 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1026 22:12:50.244641   41760 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1026 22:12:50.252691   41760 start.go:563] Will wait 60s for crictl version
I1026 22:12:50.254287   41760 ssh_runner.go:195] Run: which crictl
I1026 22:12:50.265300   41760 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1026 22:12:50.345227   41760 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1026 22:12:50.350784   41760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1026 22:12:50.399029   41760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1026 22:12:50.458852   41760 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1026 22:12:50.468174   41760 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1026 22:12:50.674098   41760 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1026 22:12:50.675805   41760 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1026 22:12:50.683636   41760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1026 22:12:50.711728   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1026 22:12:50.787046   41760 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1026 22:12:50.787589   41760 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1026 22:12:50.792309   41760 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1026 22:12:50.832413   41760 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1026 22:12:50.832413   41760 docker.go:621] Images already preloaded, skipping extraction
I1026 22:12:50.837127   41760 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1026 22:12:50.871431   41760 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1026 22:12:50.871431   41760 cache_images.go:85] Images are preloaded, skipping loading
I1026 22:12:50.871431   41760 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1026 22:12:50.873041   41760 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1026 22:12:50.880006   41760 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1026 22:12:50.975427   41760 cni.go:84] Creating CNI manager for ""
I1026 22:12:50.975427   41760 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1026 22:12:50.975958   41760 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1026 22:12:50.975958   41760 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1026 22:12:50.975958   41760 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1026 22:12:50.979243   41760 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1026 22:12:50.999533   41760 binaries.go:44] Found k8s binaries, skipping transfer
I1026 22:12:51.002223   41760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1026 22:12:51.020041   41760 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1026 22:12:51.052129   41760 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1026 22:12:51.085397   41760 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I1026 22:12:51.120479   41760 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1026 22:12:51.127728   41760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1026 22:12:51.152666   41760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1026 22:12:51.249478   41760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1026 22:12:51.308972   41760 certs.go:68] Setting up C:\Users\nika1\.minikube\profiles\minikube for IP: 192.168.49.2
I1026 22:12:51.308972   41760 certs.go:194] generating shared ca certs ...
I1026 22:12:51.308972   41760 certs.go:226] acquiring lock for ca certs: {Name:mk5962ad12378631ea3d85abe47abf408ee99b81 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:12:51.321681   41760 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\nika1\.minikube\ca.key
I1026 22:12:51.343260   41760 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\nika1\.minikube\proxy-client-ca.key
I1026 22:12:51.343260   41760 certs.go:256] generating profile certs ...
I1026 22:12:51.343962   41760 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\nika1\.minikube\profiles\minikube\client.key
I1026 22:12:51.344522   41760 crypto.go:68] Generating cert C:\Users\nika1\.minikube\profiles\minikube\client.crt with IP's: []
I1026 22:12:51.761232   41760 crypto.go:156] Writing cert to C:\Users\nika1\.minikube\profiles\minikube\client.crt ...
I1026 22:12:51.761232   41760 lock.go:35] WriteFile acquiring C:\Users\nika1\.minikube\profiles\minikube\client.crt: {Name:mk2e55bae935e2a96514ed2da9a6bf01b476dc22 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:12:51.763231   41760 crypto.go:164] Writing key to C:\Users\nika1\.minikube\profiles\minikube\client.key ...
I1026 22:12:51.763231   41760 lock.go:35] WriteFile acquiring C:\Users\nika1\.minikube\profiles\minikube\client.key: {Name:mk1c5f8449abb95152f8ade35347193ae4dc27e5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:12:51.764234   41760 certs.go:363] generating signed profile cert for "minikube": C:\Users\nika1\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1026 22:12:51.764234   41760 crypto.go:68] Generating cert C:\Users\nika1\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1026 22:12:51.978225   41760 crypto.go:156] Writing cert to C:\Users\nika1\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I1026 22:12:51.978225   41760 lock.go:35] WriteFile acquiring C:\Users\nika1\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkbda999939ae8368b14fe86999ff448c74e3102 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:12:51.979243   41760 crypto.go:164] Writing key to C:\Users\nika1\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I1026 22:12:51.979243   41760 lock.go:35] WriteFile acquiring C:\Users\nika1\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk41120852f8c2be802e21cc6d83fa4be5989efa Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:12:51.981239   41760 certs.go:381] copying C:\Users\nika1\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\nika1\.minikube\profiles\minikube\apiserver.crt
I1026 22:12:51.993755   41760 certs.go:385] copying C:\Users\nika1\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\nika1\.minikube\profiles\minikube\apiserver.key
I1026 22:12:51.994753   41760 certs.go:363] generating signed profile cert for "aggregator": C:\Users\nika1\.minikube\profiles\minikube\proxy-client.key
I1026 22:12:51.994753   41760 crypto.go:68] Generating cert C:\Users\nika1\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I1026 22:12:52.168400   41760 crypto.go:156] Writing cert to C:\Users\nika1\.minikube\profiles\minikube\proxy-client.crt ...
I1026 22:12:52.168400   41760 lock.go:35] WriteFile acquiring C:\Users\nika1\.minikube\profiles\minikube\proxy-client.crt: {Name:mk64e2cae69424ba6fd504dd54b8f77a654614e5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:12:52.169437   41760 crypto.go:164] Writing key to C:\Users\nika1\.minikube\profiles\minikube\proxy-client.key ...
I1026 22:12:52.169437   41760 lock.go:35] WriteFile acquiring C:\Users\nika1\.minikube\profiles\minikube\proxy-client.key: {Name:mk7cc9a21eba974b890e28546db77609aebd1e54 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:12:52.181529   41760 certs.go:484] found cert: C:\Users\nika1\.minikube\certs\ca-key.pem (1675 bytes)
I1026 22:12:52.181529   41760 certs.go:484] found cert: C:\Users\nika1\.minikube\certs\ca.pem (1074 bytes)
I1026 22:12:52.182536   41760 certs.go:484] found cert: C:\Users\nika1\.minikube\certs\cert.pem (1119 bytes)
I1026 22:12:52.182536   41760 certs.go:484] found cert: C:\Users\nika1\.minikube\certs\key.pem (1679 bytes)
I1026 22:12:52.189951   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1026 22:12:52.234722   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1026 22:12:52.285796   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1026 22:12:52.334328   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1026 22:12:52.383758   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1026 22:12:52.432717   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1026 22:12:52.478577   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1026 22:12:52.538020   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1026 22:12:52.581672   41760 ssh_runner.go:362] scp C:\Users\nika1\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1026 22:12:52.631406   41760 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1026 22:12:52.670001   41760 ssh_runner.go:195] Run: openssl version
I1026 22:12:52.689112   41760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1026 22:12:52.711689   41760 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1026 22:12:52.720175   41760 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 16 06:47 /usr/share/ca-certificates/minikubeCA.pem
I1026 22:12:52.720708   41760 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1026 22:12:52.737418   41760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1026 22:12:52.759592   41760 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1026 22:12:52.767620   41760 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1026 22:12:52.767620   41760 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1026 22:12:52.774644   41760 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1026 22:12:52.812381   41760 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1026 22:12:52.833888   41760 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1026 22:12:52.852136   41760 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1026 22:12:52.854742   41760 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1026 22:12:52.873236   41760 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1026 22:12:52.873236   41760 kubeadm.go:157] found existing configuration files:

I1026 22:12:52.875850   41760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1026 22:12:52.894268   41760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1026 22:12:52.896911   41760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1026 22:12:52.916321   41760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1026 22:12:52.933439   41760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1026 22:12:52.936155   41760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1026 22:12:52.956222   41760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1026 22:12:52.973318   41760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1026 22:12:52.976513   41760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1026 22:12:52.997346   41760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1026 22:12:53.014593   41760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1026 22:12:53.017845   41760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1026 22:12:53.034315   41760 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1026 22:12:53.436571   41760 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1026 22:12:53.543637   41760 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1026 22:13:10.977628   41760 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I1026 22:13:10.977628   41760 kubeadm.go:310] [preflight] Running pre-flight checks
I1026 22:13:10.978156   41760 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1026 22:13:10.978156   41760 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1026 22:13:10.978156   41760 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1026 22:13:10.978156   41760 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1026 22:13:11.029015   41760 out.go:252]     ‚ñ™ Generating certificates and keys ...
I1026 22:13:11.029526   41760 kubeadm.go:310] [certs] Using existing ca certificate authority
I1026 22:13:11.029526   41760 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1026 22:13:11.029526   41760 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1026 22:13:11.030088   41760 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1026 22:13:11.030088   41760 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1026 22:13:11.030088   41760 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1026 22:13:11.030088   41760 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1026 22:13:11.030088   41760 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1026 22:13:11.030618   41760 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1026 22:13:11.030618   41760 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1026 22:13:11.030618   41760 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1026 22:13:11.030618   41760 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1026 22:13:11.030618   41760 kubeadm.go:310] [certs] Generating "sa" key and public key
I1026 22:13:11.030618   41760 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1026 22:13:11.031168   41760 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1026 22:13:11.031168   41760 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1026 22:13:11.031168   41760 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1026 22:13:11.031168   41760 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1026 22:13:11.031168   41760 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1026 22:13:11.031168   41760 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1026 22:13:11.031168   41760 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1026 22:13:11.045716   41760 out.go:252]     ‚ñ™ Booting up control plane ...
I1026 22:13:11.046815   41760 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1026 22:13:11.046815   41760 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1026 22:13:11.047521   41760 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1026 22:13:11.047521   41760 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1026 22:13:11.047521   41760 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I1026 22:13:11.048070   41760 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I1026 22:13:11.048070   41760 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1026 22:13:11.048070   41760 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1026 22:13:11.048070   41760 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1026 22:13:11.048630   41760 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1026 22:13:11.048630   41760 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.013758577s
I1026 22:13:11.048630   41760 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I1026 22:13:11.048630   41760 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I1026 22:13:11.049169   41760 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I1026 22:13:11.049169   41760 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I1026 22:13:11.049169   41760 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 6.308905065s
I1026 22:13:11.049169   41760 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 7.387402943s
I1026 22:13:11.049708   41760 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 10.501976573s
I1026 22:13:11.049708   41760 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1026 22:13:11.049708   41760 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1026 22:13:11.049708   41760 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1026 22:13:11.050243   41760 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1026 22:13:11.050243   41760 kubeadm.go:310] [bootstrap-token] Using token: iu4am5.kmp7kvd1ujjn8313
I1026 22:13:11.064671   41760 out.go:252]     ‚ñ™ Configuring RBAC rules ...
I1026 22:13:11.065092   41760 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1026 22:13:11.065092   41760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1026 22:13:11.065603   41760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1026 22:13:11.065603   41760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1026 22:13:11.066163   41760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1026 22:13:11.066163   41760 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1026 22:13:11.066163   41760 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1026 22:13:11.066163   41760 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1026 22:13:11.066163   41760 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1026 22:13:11.066163   41760 kubeadm.go:310] 
I1026 22:13:11.066163   41760 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1026 22:13:11.066163   41760 kubeadm.go:310] 
I1026 22:13:11.066700   41760 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1026 22:13:11.066700   41760 kubeadm.go:310] 
I1026 22:13:11.066700   41760 kubeadm.go:310]   mkdir -p $HOME/.kube
I1026 22:13:11.066700   41760 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1026 22:13:11.066700   41760 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1026 22:13:11.066700   41760 kubeadm.go:310] 
I1026 22:13:11.066700   41760 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1026 22:13:11.066700   41760 kubeadm.go:310] 
I1026 22:13:11.066700   41760 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1026 22:13:11.066700   41760 kubeadm.go:310] 
I1026 22:13:11.066700   41760 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1026 22:13:11.066700   41760 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1026 22:13:11.067231   41760 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1026 22:13:11.067231   41760 kubeadm.go:310] 
I1026 22:13:11.067231   41760 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1026 22:13:11.067231   41760 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1026 22:13:11.067231   41760 kubeadm.go:310] 
I1026 22:13:11.067231   41760 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token iu4am5.kmp7kvd1ujjn8313 \
I1026 22:13:11.067750   41760 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:86b2431ff01d923c39db1dfaf3a826ee9dedd23d6171bc5dc7a9a8b8c0578443 \
I1026 22:13:11.067750   41760 kubeadm.go:310] 	--control-plane 
I1026 22:13:11.067750   41760 kubeadm.go:310] 
I1026 22:13:11.067750   41760 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1026 22:13:11.067750   41760 kubeadm.go:310] 
I1026 22:13:11.067750   41760 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token iu4am5.kmp7kvd1ujjn8313 \
I1026 22:13:11.067750   41760 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:86b2431ff01d923c39db1dfaf3a826ee9dedd23d6171bc5dc7a9a8b8c0578443 
I1026 22:13:11.067750   41760 cni.go:84] Creating CNI manager for ""
I1026 22:13:11.067750   41760 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1026 22:13:11.082063   41760 out.go:179] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1026 22:13:11.088274   41760 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1026 22:13:11.108140   41760 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1026 22:13:11.145235   41760 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1026 22:13:11.149042   41760 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1026 22:13:11.160482   41760 ops.go:34] apiserver oom_adj: -16
I1026 22:13:11.203212   41760 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_10_26T22_13_11_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1026 22:13:13.010296   41760 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_10_26T22_13_11_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (1.8070841s)
I1026 22:13:13.010296   41760 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (1.8612546s)
I1026 22:13:13.076426   41760 kubeadm.go:1105] duration metric: took 1.9311901s to wait for elevateKubeSystemPrivileges
I1026 22:13:13.077423   41760 kubeadm.go:394] duration metric: took 20.3098032s to StartCluster
I1026 22:13:13.077423   41760 settings.go:142] acquiring lock: {Name:mk82ae36814f727bb6d2e4290c7def61aaa56a14 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:13:13.077423   41760 settings.go:150] Updating kubeconfig:  C:\Users\nika1\.kube\config
I1026 22:13:13.080427   41760 lock.go:35] WriteFile acquiring C:\Users\nika1\.kube\config: {Name:mkd7f8b7c70bd169f8936a715874a52bf7d27532 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1026 22:13:13.081426   41760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1026 22:13:13.082425   41760 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1026 22:13:13.082936   41760 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1026 22:13:13.084092   41760 out.go:179] üîé  Verifying Kubernetes components...
I1026 22:13:13.083476   41760 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1026 22:13:13.085147   41760 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1026 22:13:13.085147   41760 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1026 22:13:13.088397   41760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1026 22:13:13.101383   41760 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1026 22:13:13.101912   41760 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1026 22:13:13.102472   41760 host.go:66] Checking if "minikube" exists ...
I1026 22:13:13.122609   41760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1026 22:13:13.124777   41760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1026 22:13:13.223069   41760 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1026 22:13:13.224726   41760 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1026 22:13:13.224726   41760 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1026 22:13:13.230703   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:13:13.251561   41760 addons.go:238] Setting addon default-storageclass=true in "minikube"
I1026 22:13:13.251561   41760 host.go:66] Checking if "minikube" exists ...
I1026 22:13:13.263360   41760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1026 22:13:13.305057   41760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53715 SSHKeyPath:C:\Users\nika1\.minikube\machines\minikube\id_rsa Username:docker}
I1026 22:13:13.327954   41760 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1026 22:13:13.331911   41760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1026 22:13:13.336826   41760 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1026 22:13:13.336826   41760 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1026 22:13:13.342252   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1026 22:13:13.413524   41760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53715 SSHKeyPath:C:\Users\nika1\.minikube\machines\minikube\id_rsa Username:docker}
I1026 22:13:13.623334   41760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1026 22:13:13.924213   41760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1026 22:13:14.726055   41760 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.3941439s)
I1026 22:13:14.726055   41760 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (1.3981012s)
I1026 22:13:14.730844   41760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1026 22:13:14.755936   41760 start.go:976] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I1026 22:13:14.815124   41760 api_server.go:52] waiting for apiserver process to appear ...
I1026 22:13:14.817929   41760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1026 22:13:15.504378   41760 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1026 22:13:16.159328   41760 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.235115s)
I1026 22:13:16.159328   41760 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.5359939s)
I1026 22:13:16.159328   41760 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.341399s)
I1026 22:13:16.159328   41760 api_server.go:72] duration metric: took 3.0763914s to wait for apiserver process to appear ...
I1026 22:13:16.159328   41760 api_server.go:88] waiting for apiserver healthz status ...
I1026 22:13:16.222722   41760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53714/healthz ...
I1026 22:13:16.252005   41760 api_server.go:279] https://127.0.0.1:53714/healthz returned 200:
ok
I1026 22:13:16.264338   41760 api_server.go:141] control plane version: v1.34.0
I1026 22:13:16.264338   41760 api_server.go:131] duration metric: took 105.01ms to wait for apiserver health ...
I1026 22:13:16.264338   41760 system_pods.go:43] waiting for kube-system pods to appear ...
I1026 22:13:16.786608   41760 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass
I1026 22:13:16.787718   41760 addons.go:514] duration metric: took 3.7052932s for enable addons: enabled=[storage-provisioner default-storageclass]
I1026 22:13:16.872675   41760 system_pods.go:59] 7 kube-system pods found
I1026 22:13:16.872675   41760 system_pods.go:61] "coredns-66bc5c9577-dxn6g" [d0525fd1-f847-4430-b0ef-b89583d6ed78] Pending / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1026 22:13:16.872675   41760 system_pods.go:61] "etcd-minikube" [1868cf2d-9887-4816-b519-6ae9776a8b4f] Running
I1026 22:13:16.872675   41760 system_pods.go:61] "kube-apiserver-minikube" [0c7df6fc-3f1f-4d8e-8ff7-745ddad4ff91] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1026 22:13:16.872675   41760 system_pods.go:61] "kube-controller-manager-minikube" [d1d7057e-47ee-4a03-a310-5546e55f2d97] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1026 22:13:16.872675   41760 system_pods.go:61] "kube-proxy-hztpv" [7ce37f0e-49da-40df-856b-7d946126710c] Pending / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1026 22:13:16.872675   41760 system_pods.go:61] "kube-scheduler-minikube" [d66b4aec-a2fb-4a1a-8071-61d054da2eba] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1026 22:13:16.872675   41760 system_pods.go:61] "storage-provisioner" [33195468-5970-44f5-bfa8-df1e149cb3f0] Pending
I1026 22:13:16.872675   41760 system_pods.go:74] duration metric: took 608.3371ms to wait for pod list to return data ...
I1026 22:13:16.872675   41760 kubeadm.go:578] duration metric: took 3.7897385s to wait for: map[apiserver:true system_pods:true]
I1026 22:13:16.872675   41760 node_conditions.go:102] verifying NodePressure condition ...
I1026 22:13:17.003152   41760 node_conditions.go:122] node storage ephemeral capacity is 263112772Ki
I1026 22:13:17.003152   41760 node_conditions.go:123] node cpu capacity is 12
I1026 22:13:17.034967   41760 node_conditions.go:105] duration metric: took 162.2924ms to run NodePressure ...
I1026 22:13:17.034967   41760 start.go:241] waiting for startup goroutines ...
I1026 22:13:17.034967   41760 start.go:246] waiting for cluster config update ...
I1026 22:13:17.034967   41760 start.go:255] writing updated cluster config ...
I1026 22:13:17.066635   41760 ssh_runner.go:195] Run: rm -f paused
I1026 22:13:17.589828   41760 start.go:617] kubectl: 1.30.5, cluster: 1.34.0 (minor skew: 4)
I1026 22:13:17.773424   41760 out.go:203] 
W1026 22:13:17.814075   41760 out.go:285] ‚ùó  C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.30.5, which may have incompatibilities with Kubernetes 1.34.0.
I1026 22:13:17.849050   41760 out.go:179]     ‚ñ™ Want kubectl v1.34.0? Try 'minikube kubectl -- get pods -A'
I1026 22:13:17.887155   41760 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 27 05:32:50 minikube cri-dockerd[1433]: time="2025-10-27T05:32:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:32:54 minikube cri-dockerd[1433]: time="2025-10-27T05:32:54Z" level=error msg="error getting RW layer size for container ID 'd6cbe2ab52d295128cea552a704e334e56b37f8a9eb3efc486d69a19dcad0984': Error response from daemon: No such container: d6cbe2ab52d295128cea552a704e334e56b37f8a9eb3efc486d69a19dcad0984"
Oct 27 05:32:54 minikube cri-dockerd[1433]: time="2025-10-27T05:32:54Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'd6cbe2ab52d295128cea552a704e334e56b37f8a9eb3efc486d69a19dcad0984'"
Oct 27 05:33:00 minikube cri-dockerd[1433]: time="2025-10-27T05:33:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:33:10 minikube cri-dockerd[1433]: time="2025-10-27T05:33:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:33:20 minikube dockerd[1122]: time="2025-10-27T05:33:20.289751039Z" level=info msg="ignoring event" container=95fd0af026edaebfb210eac206a8f45bafea612e5940e7e8e547c415be3b56db module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 27 05:33:20 minikube cri-dockerd[1433]: time="2025-10-27T05:33:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:33:26 minikube cri-dockerd[1433]: time="2025-10-27T05:33:26Z" level=error msg="error getting RW layer size for container ID 'ecc12338ceb6577a5dcd5190cffe84747ac93824ff765baefbd03d04a37589a8': Error response from daemon: No such container: ecc12338ceb6577a5dcd5190cffe84747ac93824ff765baefbd03d04a37589a8"
Oct 27 05:33:26 minikube cri-dockerd[1433]: time="2025-10-27T05:33:26Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'ecc12338ceb6577a5dcd5190cffe84747ac93824ff765baefbd03d04a37589a8'"
Oct 27 05:33:30 minikube cri-dockerd[1433]: time="2025-10-27T05:33:30Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24 because it exceeded image pull deadline 1m0s. Latest progress 6bff562a7abf: Pulling fs layer "
Oct 27 05:33:30 minikube dockerd[1122]: time="2025-10-27T05:33:30.730666000Z" level=info msg="Not continuing with pull after error" error="context canceled"
Oct 27 05:37:34 minikube cri-dockerd[1433]: time="2025-10-27T05:37:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:37:44 minikube cri-dockerd[1433]: time="2025-10-27T05:37:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:37:54 minikube cri-dockerd[1433]: time="2025-10-27T05:37:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:38:04 minikube cri-dockerd[1433]: time="2025-10-27T05:38:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:38:14 minikube cri-dockerd[1433]: time="2025-10-27T05:38:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:38:24 minikube cri-dockerd[1433]: time="2025-10-27T05:38:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:38:34 minikube cri-dockerd[1433]: time="2025-10-27T05:38:34Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24 because it exceeded image pull deadline 1m0s. Latest progress 6bff562a7abf: Pulling fs layer "
Oct 27 05:38:34 minikube dockerd[1122]: time="2025-10-27T05:38:34.756092909Z" level=info msg="Not continuing with pull after error" error="context canceled"
Oct 27 05:38:48 minikube cri-dockerd[1433]: time="2025-10-27T05:38:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:38:50 minikube dockerd[1122]: time="2025-10-27T05:38:50.261944803Z" level=info msg="ignoring event" container=6aa058ab27e371625a0a21e207a8a7632f6f52d66a612836691c041daf33986e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 27 05:38:56 minikube cri-dockerd[1433]: time="2025-10-27T05:38:56Z" level=error msg="error getting RW layer size for container ID '95fd0af026edaebfb210eac206a8f45bafea612e5940e7e8e547c415be3b56db': Error response from daemon: No such container: 95fd0af026edaebfb210eac206a8f45bafea612e5940e7e8e547c415be3b56db"
Oct 27 05:38:56 minikube cri-dockerd[1433]: time="2025-10-27T05:38:56Z" level=error msg="Set backoffDuration to : 1m0s for container ID '95fd0af026edaebfb210eac206a8f45bafea612e5940e7e8e547c415be3b56db'"
Oct 27 05:38:58 minikube cri-dockerd[1433]: time="2025-10-27T05:38:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:39:08 minikube cri-dockerd[1433]: time="2025-10-27T05:39:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:39:18 minikube cri-dockerd[1433]: time="2025-10-27T05:39:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:39:20 minikube dockerd[1122]: time="2025-10-27T05:39:20.257364638Z" level=info msg="ignoring event" container=8c953419a1c527b3798ee50f5e009a17bfdbb13a3b613186019477923ad42e05 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 27 05:39:28 minikube cri-dockerd[1433]: time="2025-10-27T05:39:28Z" level=error msg="error getting RW layer size for container ID '6aa058ab27e371625a0a21e207a8a7632f6f52d66a612836691c041daf33986e': Error response from daemon: No such container: 6aa058ab27e371625a0a21e207a8a7632f6f52d66a612836691c041daf33986e"
Oct 27 05:39:28 minikube cri-dockerd[1433]: time="2025-10-27T05:39:28Z" level=error msg="Set backoffDuration to : 1m0s for container ID '6aa058ab27e371625a0a21e207a8a7632f6f52d66a612836691c041daf33986e'"
Oct 27 05:39:28 minikube cri-dockerd[1433]: time="2025-10-27T05:39:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:39:38 minikube cri-dockerd[1433]: time="2025-10-27T05:39:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:39:48 minikube cri-dockerd[1433]: time="2025-10-27T05:39:48Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24 because it exceeded image pull deadline 1m0s. Latest progress 6bff562a7abf: Pulling fs layer "
Oct 27 05:39:48 minikube dockerd[1122]: time="2025-10-27T05:39:48.734260924Z" level=info msg="Not continuing with pull after error" error="context canceled"
Oct 27 05:43:52 minikube cri-dockerd[1433]: time="2025-10-27T05:43:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:44:02 minikube cri-dockerd[1433]: time="2025-10-27T05:44:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:44:12 minikube cri-dockerd[1433]: time="2025-10-27T05:44:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:44:22 minikube cri-dockerd[1433]: time="2025-10-27T05:44:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:44:32 minikube cri-dockerd[1433]: time="2025-10-27T05:44:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:44:42 minikube cri-dockerd[1433]: time="2025-10-27T05:44:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:44:50 minikube dockerd[1122]: time="2025-10-27T05:44:50.288548295Z" level=info msg="ignoring event" container=cf9078e171f59b3d57583f5efd7e014efeaa34172629f9f2467b68f262827733 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 27 05:44:52 minikube cri-dockerd[1433]: time="2025-10-27T05:44:52Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24 because it exceeded image pull deadline 1m0s. Latest progress 6bff562a7abf: Pulling fs layer "
Oct 27 05:44:52 minikube dockerd[1122]: time="2025-10-27T05:44:52.733082617Z" level=info msg="Not continuing with pull after error" error="context canceled"
Oct 27 05:44:58 minikube cri-dockerd[1433]: time="2025-10-27T05:44:58Z" level=error msg="error getting RW layer size for container ID '8c953419a1c527b3798ee50f5e009a17bfdbb13a3b613186019477923ad42e05': Error response from daemon: No such container: 8c953419a1c527b3798ee50f5e009a17bfdbb13a3b613186019477923ad42e05"
Oct 27 05:44:58 minikube cri-dockerd[1433]: time="2025-10-27T05:44:58Z" level=error msg="Set backoffDuration to : 1m0s for container ID '8c953419a1c527b3798ee50f5e009a17bfdbb13a3b613186019477923ad42e05'"
Oct 27 05:45:03 minikube cri-dockerd[1433]: time="2025-10-27T05:45:03Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:45:13 minikube cri-dockerd[1433]: time="2025-10-27T05:45:13Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:45:20 minikube dockerd[1122]: time="2025-10-27T05:45:20.242519471Z" level=info msg="ignoring event" container=e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 27 05:45:23 minikube cri-dockerd[1433]: time="2025-10-27T05:45:23Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:45:33 minikube cri-dockerd[1433]: time="2025-10-27T05:45:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:45:43 minikube cri-dockerd[1433]: time="2025-10-27T05:45:43Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:45:53 minikube cri-dockerd[1433]: time="2025-10-27T05:45:53Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:46:03 minikube cri-dockerd[1433]: time="2025-10-27T05:46:03Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24 because it exceeded image pull deadline 1m0s. Latest progress 6bff562a7abf: Pulling fs layer "
Oct 27 05:46:03 minikube dockerd[1122]: time="2025-10-27T05:46:03.025433897Z" level=info msg="Not continuing with pull after error" error="context canceled"
Oct 27 05:50:14 minikube cri-dockerd[1433]: time="2025-10-27T05:50:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:50:24 minikube cri-dockerd[1433]: time="2025-10-27T05:50:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:50:34 minikube cri-dockerd[1433]: time="2025-10-27T05:50:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:50:44 minikube cri-dockerd[1433]: time="2025-10-27T05:50:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:51:35 minikube dockerd[1122]: time="2025-10-27T05:51:35.120646784Z" level=info msg="ignoring event" container=7b3f511f38ece1a13620c323cd125b568a0402c3a551f0ce7474092ed4bbe929 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 27 05:51:39 minikube cri-dockerd[1433]: time="2025-10-27T05:51:39Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "
Oct 27 05:51:49 minikube cri-dockerd[1433]: time="2025-10-27T05:51:49Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Pulling fs layer "


==> container status <==
CONTAINER           IMAGE                                                                                                                                           CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
6d95e151f4e37       ecef539ec78ad                                                                                                                                   24 seconds ago       Running             ingress-controller        23                  32eb1e5f00750       ingress-kong-58bccc89cb-xwzcm
7b3f511f38ece       ecef539ec78ad                                                                                                                                   About a minute ago   Exited              ingress-controller        22                  32eb1e5f00750       ingress-kong-58bccc89cb-xwzcm
861e0e6709678       kong@sha256:14c689c0caf1b8da1403a742016ec64d2f5b5b12ecdec2989f36b2c2c4aa1ac0                                                                    12 hours ago         Running             proxy                     0                   ed0bcec38ff03       proxy-kong-7fd987499-wjfmp
8137e3670b497       ghcr.io/veronikashevchuk/architecture-cinemaabyss-main/proxy@sha256:57c2e267bea193ec736afdc2e6742280c2f19c6e567415347a51f36cfa38d74a            13 hours ago         Running             proxy-service             0                   ac765f56f6a1d       proxy-service-5594dcb896-8h6tn
cf376186774e2       ghcr.io/veronikashevchuk/architecture-cinemaabyss-main/proxy@sha256:57c2e267bea193ec736afdc2e6742280c2f19c6e567415347a51f36cfa38d74a            13 hours ago         Running             proxy-service             0                   e52cfea7bea44       proxy-service-5594dcb896-28dhb
2364f003f078f       ghcr.io/veronikashevchuk/architecture-cinemaabyss-main/events-service@sha256:ef930a1717d5e1c7d1132ba4f76b9b96b4d67136b78a2f1dcf9234ae8ed3aabf   13 hours ago         Running             events-service            0                   d0af71c4e1917       events-service-795948498b-dmdnp
fb15c020be8ef       ghcr.io/veronikashevchuk/architecture-cinemaabyss-main/movies-service@sha256:861e17420e3df6598236b7a5758f1133ab5af017590cc79f52079d4d3504de19   15 hours ago         Running             movies-service            0                   fb8dfd253e25b       movies-service-77cb944b49-ktxql
9d30f871f669b       ghcr.io/veronikashevchuk/architecture-cinemaabyss-main/monolith@sha256:ecc1634e30d6afcea2e166c581727ff6b4d1971d69f35bf5278c28b5ea7d0121         15 hours ago         Running             monolith                  0                   8cb832293bff1       monolith-767864457d-chblg
725d10ba86881       wurstmeister/kafka@sha256:4bad02cf8f07d0bf65d5cc73cce7aa75f9a90e32b585f867fce7c3fff229bd6d                                                      15 hours ago         Running             kafka                     0                   5b6621cd11343       kafka-0
7ce4aec0a1d0b       wurstmeister/zookeeper@sha256:7a7fd44a72104bfbd24a77844bad5fabc86485b036f988ea927d1780782a6680                                                  15 hours ago         Running             zookeeper                 0                   a93bb5b7237bd       zookeeper-0
68e466308daf8       postgres@sha256:d35fe6e6bd8d17d66dc383e18f68ce73bc9772cd3121eaa96e2c7944fe17d337                                                                15 hours ago         Running             postgres                  0                   4cea9fadcafb0       postgres-0
09cb1db616ae8       52546a367cc9e                                                                                                                                   15 hours ago         Running             coredns                   0                   e8402446fafbf       coredns-66bc5c9577-dxn6g
a4206373e0df7       6e38f40d628db                                                                                                                                   15 hours ago         Running             storage-provisioner       0                   60c88f5c5c208       storage-provisioner
470a32799a39a       df0860106674d                                                                                                                                   15 hours ago         Running             kube-proxy                0                   84a60f798f9b8       kube-proxy-hztpv
80eddb6a25559       46169d968e920                                                                                                                                   15 hours ago         Running             kube-scheduler            0                   5217dcab0e0cb       kube-scheduler-minikube
f2e11204bd5ad       a0af72f2ec6d6                                                                                                                                   15 hours ago         Running             kube-controller-manager   0                   cec48084c9b5b       kube-controller-manager-minikube
97c6c4d0e9225       5f1f5298c888d                                                                                                                                   15 hours ago         Running             etcd                      0                   0f6cf7ef71b99       etcd-minikube
5d6412dbc0f6e       90550c43ad2bc                                                                                                                                   15 hours ago         Running             kube-apiserver            0                   2597918872acb       kube-apiserver-minikube


==> coredns [09cb1db616ae] <==
[INFO] 10.244.0.5:39731 - 35228 "A IN zookeeper.cinemaabyss.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000429403s
[INFO] 10.244.0.25:33628 - 24600 "A IN kafka.cinemaabyss.svc.cluster.local.svc.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000234019s
[INFO] 10.244.0.25:33628 - 32260 "AAAA IN kafka.cinemaabyss.svc.cluster.local.svc.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000393712s
[INFO] 10.244.0.25:51380 - 47909 "A IN kafka.cinemaabyss.svc.cluster.local.cinemaabyss.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.003693509s
[INFO] 10.244.0.25:51380 - 50215 "AAAA IN kafka.cinemaabyss.svc.cluster.local.cinemaabyss.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.003704812s
[INFO] 10.244.0.25:48986 - 50335 "AAAA IN kafka.cinemaabyss.svc.cluster.local.cluster.local. udp 67 false 512" NXDOMAIN qr,aa,rd 160 0.000208008s
[INFO] 10.244.0.25:48986 - 18333 "A IN kafka.cinemaabyss.svc.cluster.local.cluster.local. udp 67 false 512" NXDOMAIN qr,aa,rd 160 0.000316371s
[INFO] 10.244.0.25:56418 - 1424 "AAAA IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 146 0.000399704s
[INFO] 10.244.0.25:56418 - 41884 "A IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 104 0.000510673s
[INFO] 10.244.0.25:53969 - 33052 "AAAA IN kafka.cinemaabyss.svc.cluster.local.cinemaabyss.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.00022405s
[INFO] 10.244.0.25:53969 - 13338 "A IN kafka.cinemaabyss.svc.cluster.local.cinemaabyss.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.000414393s
[INFO] 10.244.0.25:54272 - 6165 "AAAA IN kafka.cinemaabyss.svc.cluster.local.svc.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000276422s
[INFO] 10.244.0.25:54272 - 5915 "A IN kafka.cinemaabyss.svc.cluster.local.svc.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000349686s
[INFO] 10.244.0.25:44748 - 14387 "AAAA IN kafka.cinemaabyss.svc.cluster.local.cluster.local. udp 67 false 512" NXDOMAIN qr,aa,rd 160 0.000195263s
[INFO] 10.244.0.25:44748 - 24381 "A IN kafka.cinemaabyss.svc.cluster.local.cluster.local. udp 67 false 512" NXDOMAIN qr,aa,rd 160 0.000299588s
[INFO] 10.244.0.25:56644 - 50809 "AAAA IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 146 0.000135366s
[INFO] 10.244.0.25:56644 - 62327 "A IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 104 0.000237776s
[INFO] 10.244.0.26:51807 - 33734 "A IN kafka.cinemaabyss.svc.cluster.local.cinemaabyss.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.00332814s
[INFO] 10.244.0.26:51807 - 33476 "AAAA IN kafka.cinemaabyss.svc.cluster.local.cinemaabyss.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.003193459s
[INFO] 10.244.0.26:60081 - 40541 "A IN kafka.cinemaabyss.svc.cluster.local.svc.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.000298176s
[INFO] 10.244.0.26:60081 - 47705 "AAAA IN kafka.cinemaabyss.svc.cluster.local.svc.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.00040931s
[INFO] 10.244.0.26:39446 - 45104 "AAAA IN kafka.cinemaabyss.svc.cluster.local.cluster.local. udp 67 false 512" NXDOMAIN qr,aa,rd 160 0.000228599s
[INFO] 10.244.0.26:39446 - 52534 "A IN kafka.cinemaabyss.svc.cluster.local.cluster.local. udp 67 false 512" NXDOMAIN qr,aa,rd 160 0.000319739s
[INFO] 10.244.0.26:42244 - 64446 "AAAA IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 146 0.000285631s
[INFO] 10.244.0.26:42244 - 13247 "A IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 104 0.000370218s
[INFO] 10.244.0.26:57149 - 46225 "AAAA IN kafka.cinemaabyss.svc.cluster.local.cinemaabyss.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.000298678s
[INFO] 10.244.0.26:57149 - 55964 "A IN kafka.cinemaabyss.svc.cluster.local.cinemaabyss.svc.cluster.local. udp 83 false 512" NXDOMAIN qr,aa,rd 176 0.000470255s
[INFO] 10.244.0.26:37914 - 37378 "AAAA IN kafka.cinemaabyss.svc.cluster.local.svc.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.00024936s
[INFO] 10.244.0.26:37914 - 28685 "A IN kafka.cinemaabyss.svc.cluster.local.svc.cluster.local. udp 71 false 512" NXDOMAIN qr,aa,rd 164 0.0003761s
[INFO] 10.244.0.26:47695 - 53486 "AAAA IN kafka.cinemaabyss.svc.cluster.local.cluster.local. udp 67 false 512" NXDOMAIN qr,aa,rd 160 0.000171197s
[INFO] 10.244.0.26:47695 - 36320 "A IN kafka.cinemaabyss.svc.cluster.local.cluster.local. udp 67 false 512" NXDOMAIN qr,aa,rd 160 0.000251124s
[INFO] 10.244.0.26:33521 - 12837 "AAAA IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 146 0.000231856s
[INFO] 10.244.0.26:33521 - 11559 "A IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 104 0.000312545s
[INFO] 10.244.0.26:58151 - 40757 "A IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 104 0.000674277s
[INFO] 10.244.0.26:58151 - 29490 "AAAA IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 146 0.000821122s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.373845464s
[INFO] 10.244.0.26:57258 - 7682 "A IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 104 0.013415391s
[INFO] 10.244.0.26:57258 - 63247 "AAAA IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 146 0.013423838s
[INFO] 10.244.0.42:51791 - 23585 "SRV IN kong-hf.konghq.com.kong.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.005052966s
[INFO] 10.244.0.42:51349 - 11077 "SRV IN kong-hf.konghq.com.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.001715991s
[INFO] 10.244.0.42:58222 - 38565 "SRV IN kong-hf.konghq.com.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000350668s
[INFO] 10.244.0.42:44377 - 19631 "SRV IN kong-hf.konghq.com. udp 36 false 512" NOERROR qr,rd,ra 36 0.279350714s
[INFO] 10.244.0.42:57781 - 51269 "A IN kong-hf.konghq.com.kong.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000394235s
[INFO] 10.244.0.42:56144 - 52708 "A IN kong-hf.konghq.com.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000471186s
[INFO] 10.244.0.42:56301 - 4520 "A IN kong-hf.konghq.com.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.007294688s
[INFO] 10.244.0.42:37405 - 33246 "A IN kong-hf.konghq.com. udp 36 false 512" NOERROR qr,rd,ra 104 0.019878195s
[INFO] 10.244.0.26:34623 - 62727 "A IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 104 0.000454362s
[INFO] 10.244.0.26:34623 - 27656 "AAAA IN kafka.cinemaabyss.svc.cluster.local. udp 53 false 512" NOERROR qr,aa,rd 146 0.00064633s
[INFO] 10.244.0.42:33098 - 23466 "SRV IN kong-hf.konghq.com.kong.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000245974s
[INFO] 10.244.0.42:60080 - 22401 "SRV IN kong-hf.konghq.com.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000348093s
[INFO] 10.244.0.42:50175 - 59902 "SRV IN kong-hf.konghq.com.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000296282s
[INFO] 10.244.0.42:38215 - 14160 "SRV IN kong-hf.konghq.com.kong.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000274018s
[INFO] 10.244.0.42:41175 - 56127 "SRV IN kong-hf.konghq.com.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000312003s
[INFO] 10.244.0.42:40758 - 2994 "SRV IN kong-hf.konghq.com.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000251884s
[INFO] 10.244.0.42:44025 - 9787 "SRV IN kong-hf.konghq.com. udp 36 false 512" NOERROR qr,rd,ra 36 0.359835561s
[INFO] 10.244.0.42:41927 - 49676 "A IN kong-hf.konghq.com.kong.svc.cluster.local. udp 59 false 512" NXDOMAIN qr,aa,rd 152 0.000310299s
[INFO] 10.244.0.42:57833 - 16490 "A IN kong-hf.konghq.com.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000271172s
[INFO] 10.244.0.42:46760 - 20206 "A IN kong-hf.konghq.com.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000535762s
[INFO] 10.244.0.42:47449 - 16950 "A IN kong-hf.konghq.com. udp 36 false 512" NOERROR qr,rd,ra 104 0.105797113s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_10_26T22_13_11_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 26 Oct 2025 15:13:05 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 27 Oct 2025 05:51:51 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 27 Oct 2025 05:48:55 +0000   Sun, 26 Oct 2025 15:13:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 27 Oct 2025 05:48:55 +0000   Sun, 26 Oct 2025 15:13:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 27 Oct 2025 05:48:55 +0000   Sun, 26 Oct 2025 15:13:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 27 Oct 2025 05:48:55 +0000   Sun, 26 Oct 2025 15:13:06 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7781584Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  263112772Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7781584Ki
  pods:               110
System Info:
  Machine ID:                 b2b0fd952c93436cb6ac576de7c0d932
  System UUID:                b2b0fd952c93436cb6ac576de7c0d932
  Boot ID:                    127f0cf4-ab94-44cc-b12f-3cbfd2658a2d
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (20 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  cinemaabyss                 events-service-795948498b-dmdnp              200m (1%)     500m (4%)   256Mi (3%)       512Mi (6%)     12h
  cinemaabyss                 kafka-0                                      200m (1%)     1 (8%)      512Mi (6%)       1Gi (13%)      14h
  cinemaabyss                 monolith-767864457d-chblg                    100m (0%)     500m (4%)   128Mi (1%)       512Mi (6%)     14h
  cinemaabyss                 movies-service-77cb944b49-ktxql              100m (0%)     300m (2%)   128Mi (1%)       256Mi (3%)     14h
  cinemaabyss                 postgres-0                                   500m (4%)     1 (8%)      512Mi (6%)       1Gi (13%)      14h
  cinemaabyss                 proxy-service-5594dcb896-28dhb               100m (0%)     200m (1%)   128Mi (1%)       256Mi (3%)     12h
  cinemaabyss                 proxy-service-5594dcb896-8h6tn               100m (0%)     200m (1%)   128Mi (1%)       256Mi (3%)     12h
  cinemaabyss                 zookeeper-0                                  100m (0%)     500m (4%)   256Mi (3%)       512Mi (6%)     14h
  ingress-nginx               ingress-nginx-admission-create-hvvg7         0 (0%)        0 (0%)      0 (0%)           0 (0%)         11h
  ingress-nginx               ingress-nginx-admission-patch-b8dct          0 (0%)        0 (0%)      0 (0%)           0 (0%)         11h
  ingress-nginx               ingress-nginx-controller-77c695954b-mxl8b    100m (0%)     0 (0%)      90Mi (1%)        0 (0%)         11h
  kong                        ingress-kong-58bccc89cb-xwzcm                0 (0%)        0 (0%)      0 (0%)           0 (0%)         11h
  kong                        proxy-kong-7fd987499-wjfmp                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         11h
  kube-system                 coredns-66bc5c9577-dxn6g                     100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     14h
  kube-system                 etcd-minikube                                100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         14h
  kube-system                 kube-apiserver-minikube                      250m (2%)     0 (0%)      0 (0%)           0 (0%)         14h
  kube-system                 kube-controller-manager-minikube             200m (1%)     0 (0%)      0 (0%)           0 (0%)         14h
  kube-system                 kube-proxy-hztpv                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         14h
  kube-system                 kube-scheduler-minikube                      100m (0%)     0 (0%)      0 (0%)           0 (0%)         14h
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         14h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                2250m (18%)   4200m (35%)
  memory             2308Mi (30%)  4522Mi (59%)
  ephemeral-storage  0 (0%)        0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
Events:              <none>


==> dmesg <==
[Oct27 00:44] Speculative Return Stack Overflow: IBPB-extending microcode not applied!
[  +0.000000] Speculative Return Stack Overflow: WARNING: See https://kernel.org/doc/html/latest/admin-guide/hw-vuln/srso.html for mitigation options.
[  +0.048226] PCI: Fatal: No config space access function found
[  +0.036159] PCI: System does not support PCI
[  +0.162321] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[Oct27 00:45] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2058: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.017981] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Novosibirsk not found. Is the tzdata package installed?
[  +0.245680] pulseaudio[280]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.188438] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.009541] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000816] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000862] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000994] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.941908] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.008165] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000940] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001241] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001589] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.374811] Failed to connect to bus: No such file or directory
[  +0.654086] systemd-journald[39]: File /var/log/journal/e54490b30b914a229c1dfd824ac95114/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +1.663760] WSL (241) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +1.704185] netlink: 'init': attribute type 4 has an invalid length.
[Oct27 00:55] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +0.369502] block sda: the capability attribute has been deprecated.
[Oct27 01:17] hrtimer: interrupt took 6016379 ns
[ +14.446353] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Novosibirsk not found. Is the tzdata package installed?
[Oct27 01:18] WSL (241) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct27 01:23] WSL (241) ERROR: CheckConnection: getaddrinfo() failed: -5
[Oct27 01:28] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Novosibirsk not found. Is the tzdata package installed?
[Oct27 01:35] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Novosibirsk not found. Is the tzdata package installed?
[Oct27 01:59] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.001308] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.597049] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000008] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.598860] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000007] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +1.096693] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000009] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.721210] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000010] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.615124] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000007] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Oct27 03:36] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Novosibirsk not found. Is the tzdata package installed?
[  +0.000586] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Novosibirsk not found. Is the tzdata package installed?
[Oct27 05:20] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Novosibirsk not found. Is the tzdata package installed?


==> etcd [97c6c4d0e922] <==
{"level":"warn","ts":"2025-10-27T05:08:19.096830Z","caller":"etcdserver/server.go:900","msg":"failed to revoke lease","lease-id":"70cc9a21147e42e5","error":"lease not found"}
{"level":"warn","ts":"2025-10-27T05:08:19.100745Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-27T05:08:18.696568Z","time spent":"399.980781ms","remote":"127.0.0.1:38612","response type":"/etcdserverpb.KV/Range","request count":0,"request size":33,"response count":0,"response size":29,"request content":"key:\"/registry/clusterrolebindings\" limit:1 "}
{"level":"info","ts":"2025-10-27T05:08:19.290112Z","caller":"traceutil/trace.go:172","msg":"trace[820195604] compact","detail":"{revision:12394; response_revision:12677; }","duration":"189.519847ms","start":"2025-10-27T05:08:19.100569Z","end":"2025-10-27T05:08:19.290089Z","steps":["trace[820195604] 'process raft request'  (duration: 96.736482ms)","trace[820195604] 'check and update compact revision'  (duration: 92.479272ms)"],"step_count":2}
{"level":"info","ts":"2025-10-27T05:08:19.290696Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":12394}
{"level":"info","ts":"2025-10-27T05:08:19.463177Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":12394,"took":"171.954896ms","hash":2609821764,"current-db-size-bytes":3211264,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2396160,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-10-27T05:08:19.463272Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2609821764,"revision":12394,"compact-revision":12039}
{"level":"warn","ts":"2025-10-27T05:08:19.463549Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"163.264733ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128040894365320023 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc9a21147e4356>","response":"size:41"}
{"level":"info","ts":"2025-10-27T05:08:19.463679Z","caller":"traceutil/trace.go:172","msg":"trace[1893562760] linearizableReadLoop","detail":"{readStateIndex:15206; appliedIndex:15204; }","duration":"158.912432ms","start":"2025-10-27T05:08:19.304757Z","end":"2025-10-27T05:08:19.463670Z","steps":["trace[1893562760] 'read index received'  (duration: 18.767¬µs)","trace[1893562760] 'applied index is now lower than readState.Index'  (duration: 158.893064ms)"],"step_count":2}
{"level":"warn","ts":"2025-10-27T05:08:19.489780Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"184.988582ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/ingress-nginx/ingress-nginx-admission\" limit:1 ","response":"range_response_count:1 size:979"}
{"level":"info","ts":"2025-10-27T05:08:19.489858Z","caller":"traceutil/trace.go:172","msg":"trace[622335659] range","detail":"{range_begin:/registry/serviceaccounts/ingress-nginx/ingress-nginx-admission; range_end:; response_count:1; response_revision:12679; }","duration":"185.092606ms","start":"2025-10-27T05:08:19.304752Z","end":"2025-10-27T05:08:19.489845Z","steps":["trace[622335659] 'agreement among raft nodes before linearized reading'  (duration: 184.79729ms)"],"step_count":1}
{"level":"info","ts":"2025-10-27T05:08:19.489855Z","caller":"traceutil/trace.go:172","msg":"trace[276314382] transaction","detail":"{read_only:false; response_revision:12679; number_of_response:1; }","duration":"187.197187ms","start":"2025-10-27T05:08:19.302626Z","end":"2025-10-27T05:08:19.489823Z","steps":["trace[276314382] 'process raft request'  (duration: 160.975031ms)"],"step_count":1}
{"level":"info","ts":"2025-10-27T05:08:28.593674Z","caller":"traceutil/trace.go:172","msg":"trace[733981900] transaction","detail":"{read_only:false; response_revision:12704; number_of_response:1; }","duration":"1.300077379s","start":"2025-10-27T05:08:27.293577Z","end":"2025-10-27T05:08:28.593654Z","steps":["trace[733981900] 'process raft request'  (duration: 1.299931533s)"],"step_count":1}
{"level":"warn","ts":"2025-10-27T05:08:28.593867Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-27T05:08:27.293554Z","time spent":"1.300227694s","remote":"127.0.0.1:38254","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:12703 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-10-27T05:13:19.286714Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":12672}
{"level":"info","ts":"2025-10-27T05:13:19.294601Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":12672,"took":"7.365834ms","hash":663951825,"current-db-size-bytes":3211264,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2351104,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-10-27T05:13:19.294682Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":663951825,"revision":12672,"compact-revision":12394}
{"level":"info","ts":"2025-10-27T05:21:01.585296Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13005}
{"level":"info","ts":"2025-10-27T05:21:01.591954Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13005,"took":"5.864685ms","hash":993178883,"current-db-size-bytes":3211264,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2371584,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-10-27T05:21:01.592021Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":993178883,"revision":13005,"compact-revision":12672}
{"level":"info","ts":"2025-10-27T05:24:43.261095Z","caller":"traceutil/trace.go:172","msg":"trace[1805976538] linearizableReadLoop","detail":"{readStateIndex:16181; appliedIndex:16181; }","duration":"351.790955ms","start":"2025-10-27T05:24:42.909150Z","end":"2025-10-27T05:24:43.260941Z","steps":["trace[1805976538] 'read index received'  (duration: 351.780123ms)","trace[1805976538] 'applied index is now lower than readState.Index'  (duration: 8.978¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-10-27T05:24:43.261217Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"351.997321ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-27T05:24:43.261260Z","caller":"traceutil/trace.go:172","msg":"trace[488922497] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13468; }","duration":"352.104949ms","start":"2025-10-27T05:24:42.909146Z","end":"2025-10-27T05:24:43.261251Z","steps":["trace[488922497] 'agreement among raft nodes before linearized reading'  (duration: 351.942356ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-27T05:24:43.261298Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"202.773892ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-10-27T05:24:43.261283Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-27T05:24:42.909129Z","time spent":"352.146767ms","remote":"127.0.0.1:46578","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-10-27T05:24:43.261149Z","caller":"traceutil/trace.go:172","msg":"trace[522105760] transaction","detail":"{read_only:false; response_revision:13468; number_of_response:1; }","duration":"507.496842ms","start":"2025-10-27T05:24:42.753636Z","end":"2025-10-27T05:24:43.261133Z","steps":["trace[522105760] 'process raft request'  (duration: 507.336794ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-27T05:24:43.261513Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"197.670167ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-27T05:24:43.261596Z","caller":"traceutil/trace.go:172","msg":"trace[144503898] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13468; }","duration":"197.761681ms","start":"2025-10-27T05:24:43.063822Z","end":"2025-10-27T05:24:43.261584Z","steps":["trace[144503898] 'agreement among raft nodes before linearized reading'  (duration: 197.640304ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-27T05:24:43.261651Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-27T05:24:42.753617Z","time spent":"507.789119ms","remote":"127.0.0.1:38254","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13467 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-10-27T05:24:43.261321Z","caller":"traceutil/trace.go:172","msg":"trace[1940423236] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13468; }","duration":"202.80605ms","start":"2025-10-27T05:24:43.058508Z","end":"2025-10-27T05:24:43.261314Z","steps":["trace[1940423236] 'agreement among raft nodes before linearized reading'  (duration: 202.764232ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-27T05:24:43.448383Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-27T05:24:43.058487Z","time spent":"389.305761ms","remote":"127.0.0.1:46576","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-10-27T05:24:44.218622Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"160.965543ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-10-27T05:24:44.218636Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"670.371481ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-27T05:24:44.218694Z","caller":"traceutil/trace.go:172","msg":"trace[503715531] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:13468; }","duration":"161.070064ms","start":"2025-10-27T05:24:44.057609Z","end":"2025-10-27T05:24:44.218679Z","steps":["trace[503715531] 'range keys from in-memory index tree'  (duration: 160.904635ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-27T05:24:44.218744Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"155.692367ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-10-27T05:24:44.218772Z","caller":"traceutil/trace.go:172","msg":"trace[66567979] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13468; }","duration":"155.72712ms","start":"2025-10-27T05:24:44.063038Z","end":"2025-10-27T05:24:44.218765Z","steps":["trace[66567979] 'range keys from in-memory index tree'  (duration: 155.610745ms)"],"step_count":1}
{"level":"info","ts":"2025-10-27T05:24:44.218704Z","caller":"traceutil/trace.go:172","msg":"trace[1911792657] range","detail":"{range_begin:/registry/limitranges; range_end:; response_count:0; response_revision:13468; }","duration":"670.486745ms","start":"2025-10-27T05:24:43.548203Z","end":"2025-10-27T05:24:44.218690Z","steps":["trace[1911792657] 'range keys from in-memory index tree'  (duration: 670.302064ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-27T05:24:44.218887Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-27T05:24:43.548180Z","time spent":"670.694433ms","remote":"127.0.0.1:38206","response type":"/etcdserverpb.KV/Range","request count":0,"request size":25,"response count":0,"response size":29,"request content":"key:\"/registry/limitranges\" limit:1 "}
{"level":"warn","ts":"2025-10-27T05:24:44.218604Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"769.320574ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-10-27T05:24:44.219044Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"469.136046ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/cinemaabyss/kafka-data\" limit:1 ","response":"range_response_count:1 size:1613"}
{"level":"info","ts":"2025-10-27T05:24:44.219091Z","caller":"traceutil/trace.go:172","msg":"trace[2137554049] range","detail":"{range_begin:/registry/persistentvolumeclaims/cinemaabyss/kafka-data; range_end:; response_count:1; response_revision:13468; }","duration":"469.19477ms","start":"2025-10-27T05:24:43.749888Z","end":"2025-10-27T05:24:44.219083Z","steps":["trace[2137554049] 'range keys from in-memory index tree'  (duration: 468.831142ms)"],"step_count":1}
{"level":"info","ts":"2025-10-27T05:24:44.219088Z","caller":"traceutil/trace.go:172","msg":"trace[600251454] range","detail":"{range_begin:/registry/secrets; range_end:; response_count:0; response_revision:13468; }","duration":"769.81789ms","start":"2025-10-27T05:24:43.449253Z","end":"2025-10-27T05:24:44.219071Z","steps":["trace[600251454] 'range keys from in-memory index tree'  (duration: 769.136355ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-27T05:24:44.219123Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-27T05:24:43.449224Z","time spent":"769.887266ms","remote":"127.0.0.1:38104","response type":"/etcdserverpb.KV/Range","request count":0,"request size":21,"response count":0,"response size":29,"request content":"key:\"/registry/secrets\" limit:1 "}
{"level":"warn","ts":"2025-10-27T05:24:44.219122Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-10-27T05:24:43.749863Z","time spent":"469.248534ms","remote":"127.0.0.1:38236","response type":"/etcdserverpb.KV/Range","request count":0,"request size":59,"response count":1,"response size":1637,"request content":"key:\"/registry/persistentvolumeclaims/cinemaabyss/kafka-data\" limit:1 "}
{"level":"info","ts":"2025-10-27T05:26:01.570559Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13285}
{"level":"info","ts":"2025-10-27T05:26:01.575226Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13285,"took":"4.302237ms","hash":1097597387,"current-db-size-bytes":3211264,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2105344,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-10-27T05:26:01.575303Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1097597387,"revision":13285,"compact-revision":13005}
{"level":"info","ts":"2025-10-27T05:31:01.557520Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13539}
{"level":"info","ts":"2025-10-27T05:31:01.562601Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13539,"took":"4.415916ms","hash":1110388988,"current-db-size-bytes":3211264,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2031616,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-10-27T05:31:01.562694Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1110388988,"revision":13539,"compact-revision":13285}
{"level":"info","ts":"2025-10-27T05:37:17.661854Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13791}
{"level":"info","ts":"2025-10-27T05:37:17.667779Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13791,"took":"5.251885ms","hash":4033695831,"current-db-size-bytes":3211264,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2080768,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-10-27T05:37:17.667923Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4033695831,"revision":13791,"compact-revision":13539}
{"level":"info","ts":"2025-10-27T05:42:17.652492Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14048}
{"level":"info","ts":"2025-10-27T05:42:17.657423Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14048,"took":"4.350878ms","hash":3484388098,"current-db-size-bytes":3211264,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2088960,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-10-27T05:42:17.657497Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3484388098,"revision":14048,"compact-revision":13791}
{"level":"info","ts":"2025-10-27T05:42:44.497877Z","caller":"traceutil/trace.go:172","msg":"trace[947390961] transaction","detail":"{read_only:false; response_revision:14327; number_of_response:1; }","duration":"103.528408ms","start":"2025-10-27T05:42:44.394135Z","end":"2025-10-27T05:42:44.497663Z","steps":["trace[947390961] 'process raft request'  (duration: 103.323504ms)"],"step_count":1}
{"level":"info","ts":"2025-10-27T05:42:46.690903Z","caller":"traceutil/trace.go:172","msg":"trace[887985851] transaction","detail":"{read_only:false; response_revision:14330; number_of_response:1; }","duration":"184.045152ms","start":"2025-10-27T05:42:46.506842Z","end":"2025-10-27T05:42:46.690887Z","steps":["trace[887985851] 'process raft request'  (duration: 183.914214ms)"],"step_count":1}
{"level":"info","ts":"2025-10-27T05:47:17.643260Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14306}
{"level":"info","ts":"2025-10-27T05:47:17.649967Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14306,"took":"6.045025ms","hash":1639413724,"current-db-size-bytes":3211264,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2056192,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-10-27T05:47:17.650042Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1639413724,"revision":14306,"compact-revision":14048}


==> kernel <==
 05:51:59 up  5:06,  0 users,  load average: 0.60, 0.56, 0.61
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [5d6412dbc0f6] <==
I1027 05:11:00.068671       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:11:57.173009       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:12:11.971606       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:13:13.946040       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:13:19.285373       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:14:37.328249       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:14:42.747209       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:15:58.275289       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:18:52.869100       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:19:53.845139       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:19:59.261594       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:20:56.173526       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1027 05:21:09.416311       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:21:14.104508       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:22:21.748988       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:22:30.429295       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:23:28.441953       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:23:40.536546       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:24:49.834720       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:25:09.246737       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:26:10.433355       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:26:11.463114       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:27:13.984096       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:27:25.727814       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:28:43.677145       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:28:51.525868       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:30:05.930991       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:30:20.524840       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:30:56.129376       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1027 05:31:09.708585       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:33:03.343422       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:33:51.302137       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:34:06.758634       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:34:55.930975       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:35:27.955583       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:36:15.056605       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:36:44.743402       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:37:39.297463       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:37:53.236053       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:39:02.781047       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:39:19.200162       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:40:13.744863       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:40:43.534815       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:41:24.068889       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:41:58.490633       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:42:12.205296       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1027 05:42:44.017388       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:43:06.546238       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:44:12.065939       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:44:17.476331       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:45:20.964659       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:45:39.647575       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:46:50.479605       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:47:03.611294       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:48:11.252083       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:48:21.352230       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:49:31.292548       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:49:39.173150       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:51:42.264860       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1027 05:51:47.481761       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [f2e11204bd5a] <==
I1026 15:13:15.054799       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1026 15:13:15.113072       1 shared_informer.go:356] "Caches are synced" controller="job"
I1026 15:13:15.113262       1 shared_informer.go:356] "Caches are synced" controller="node"
I1026 15:13:15.113312       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1026 15:13:15.113354       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1026 15:13:15.113362       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1026 15:13:15.113368       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1026 15:13:15.113887       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1026 15:13:15.113938       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1026 15:13:15.113956       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1026 15:13:15.114058       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1026 15:13:15.114578       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1026 15:13:15.114598       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1026 15:13:15.114773       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1026 15:13:15.114779       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1026 15:13:15.114799       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1026 15:13:15.114812       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1026 15:13:15.115654       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1026 15:13:15.116847       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1026 15:13:15.117414       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1026 15:13:15.117480       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1026 15:13:15.118563       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1026 15:13:15.121263       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1026 15:13:15.127800       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1026 15:13:15.135287       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1026 15:13:15.135451       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1026 15:13:15.140721       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1026 15:13:15.141117       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1026 15:13:15.142170       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1026 15:13:15.143430       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1026 15:13:15.143597       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1026 15:13:15.143788       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1026 15:13:15.143887       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1026 15:13:15.147182       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1026 15:13:15.215370       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1026 15:13:15.251810       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1026 15:13:15.275539       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1026 15:13:15.275621       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1026 15:13:15.275641       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1026 16:11:46.108193       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="cinemaabyss/events-service" err="EndpointSlice informer cache is out of date"
I1026 17:14:22.528005       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-zgwfq" approvedExpiration="1h0m0s"
I1026 18:12:36.555153       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1026 18:12:37.857510       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1026 18:12:37.970360       1 shared_informer.go:682] "Warning: resync period is smaller than resync check period and the informer has already started. Changing it to the resync check period" resyncPeriod="13h13m59.877208887s" resyncCheckPeriod="20h6m11.11838178s"
I1026 18:12:37.973508       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="ingressclassparameterses.configuration.konghq.com"
I1026 18:12:37.974768       1 shared_informer.go:682] "Warning: resync period is smaller than resync check period and the informer has already started. Changing it to the resync check period" resyncPeriod="13h13m10.144149766s" resyncCheckPeriod="20h6m11.11838178s"
I1026 18:12:37.975096       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="kongconsumers.configuration.konghq.com"
I1026 18:12:37.975182       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="kongplugins.configuration.konghq.com"
I1026 18:12:37.975203       1 shared_informer.go:682] "Warning: resync period is smaller than resync check period and the informer has already started. Changing it to the resync check period" resyncPeriod="19h39m30.196611148s" resyncCheckPeriod="20h6m11.11838178s"
I1026 18:12:37.975215       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="kongingresses.configuration.konghq.com"
I1026 18:12:37.975224       1 shared_informer.go:682] "Warning: resync period is smaller than resync check period and the informer has already started. Changing it to the resync check period" resyncPeriod="15h11m34.218470081s" resyncCheckPeriod="20h6m11.11838178s"
I1026 18:12:37.975234       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="tcpingresses.configuration.konghq.com"
I1026 18:12:37.975241       1 shared_informer.go:682] "Warning: resync period is smaller than resync check period and the informer has already started. Changing it to the resync check period" resyncPeriod="14h14m58.849559943s" resyncCheckPeriod="20h6m11.11838178s"
I1026 18:12:37.975251       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="udpingresses.configuration.konghq.com"
I1026 18:12:37.975444       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1026 18:12:37.975509       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1027 05:05:59.660029       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1027 05:05:59.754852       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1027 05:08:19.893850       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="cinemaabyss/proxy-service" err="failed to update proxy-service-sx8l9 EndpointSlice for Service cinemaabyss/proxy-service: Unauthorized"
I1027 05:08:19.894411       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"cinemaabyss", Name:"proxy-service", UID:"a3c770ca-51dc-4c96-8ed2-8e1b25a31bc5", APIVersion:"v1", ResourceVersion:"1010", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service cinemaabyss/proxy-service: failed to update proxy-service-sx8l9 EndpointSlice for Service cinemaabyss/proxy-service: Unauthorized


==> kube-proxy [470a32799a39] <==
I1026 15:13:24.210833       1 server_linux.go:53] "Using iptables proxy"
I1026 15:13:25.537360       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1026 15:13:25.638462       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1026 15:13:25.638555       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1026 15:13:25.638656       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1026 15:13:25.866459       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1026 15:13:25.866600       1 server_linux.go:132] "Using iptables Proxier"
I1026 15:13:25.904006       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1026 15:13:25.941288       1 server.go:527] "Version info" version="v1.34.0"
I1026 15:13:25.941358       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1026 15:13:25.944378       1 config.go:200] "Starting service config controller"
I1026 15:13:25.944416       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1026 15:13:25.944594       1 config.go:106] "Starting endpoint slice config controller"
I1026 15:13:25.944617       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1026 15:13:25.944849       1 config.go:403] "Starting serviceCIDR config controller"
I1026 15:13:25.944882       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1026 15:13:25.945201       1 config.go:309] "Starting node config controller"
I1026 15:13:25.945250       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1026 15:13:25.945259       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1026 15:13:26.044856       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1026 15:13:26.044938       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1026 15:13:26.044948       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [80eddb6a2555] <==
I1026 15:13:04.126395       1 serving.go:386] Generated self-signed cert in-memory
W1026 15:13:05.837535       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1026 15:13:05.837594       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1026 15:13:05.837609       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1026 15:13:05.837618       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1026 15:13:05.932294       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1026 15:13:05.932359       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1026 15:13:05.934887       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1026 15:13:05.934971       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1026 15:13:05.935135       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1026 15:13:05.935183       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1026 15:13:06.016238       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1026 15:13:06.017054       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1026 15:13:06.017432       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1026 15:13:06.017446       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1026 15:13:06.017562       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1026 15:13:06.017723       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1026 15:13:06.017934       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1026 15:13:06.017440       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1026 15:13:06.018217       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1026 15:13:06.018378       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1026 15:13:06.018451       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1026 15:13:06.018424       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1026 15:13:06.018839       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1026 15:13:06.018872       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1026 15:13:06.018957       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1026 15:13:06.018979       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1026 15:13:06.018994       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1026 15:13:06.019071       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1026 15:13:06.019303       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1026 15:13:06.838121       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1026 15:13:06.842733       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1026 15:13:06.869574       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1026 15:13:06.876368       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1026 15:13:06.888284       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1026 15:13:07.027797       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1026 15:13:07.033382       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1026 15:13:07.150216       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1026 15:13:07.283376       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1026 15:13:07.289132       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1026 15:13:07.327858       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1026 15:13:07.389213       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1026 15:13:07.400416       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1026 15:13:07.464057       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1026 15:13:07.486738       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1026 15:13:07.493742       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1026 15:13:07.501812       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1026 15:13:07.504473       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1026 15:13:07.619505       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
I1026 15:13:10.136171       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1026 15:15:20.432012       1 schedule_one.go:191] "Status after running PostFilter plugins for pod" logger="UnhandledError" pod="cinemaabyss/postgres-0" status="not found"
E1026 15:15:20.452586       1 schedule_one.go:191] "Status after running PostFilter plugins for pod" logger="UnhandledError" pod="cinemaabyss/postgres-0" status="not found"
E1026 15:15:20.523019       1 schedule_one.go:191] "Status after running PostFilter plugins for pod" logger="UnhandledError" pod="cinemaabyss/postgres-0" status="not found"
E1026 15:15:20.530458       1 schedule_one.go:191] "Status after running PostFilter plugins for pod" logger="UnhandledError" pod="cinemaabyss/postgres-0" status="not found"


==> kubelet <==
Oct 27 05:48:06 minikube kubelet[2378]: E1027 05:48:06.385584    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:48:06 minikube kubelet[2378]: E1027 05:48:06.388024    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:48:08 minikube kubelet[2378]: E1027 05:48:08.387406    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"
Oct 27 05:48:17 minikube kubelet[2378]: I1027 05:48:17.382755    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:48:17 minikube kubelet[2378]: E1027 05:48:17.382961    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:48:19 minikube kubelet[2378]: E1027 05:48:19.386041    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:48:20 minikube kubelet[2378]: E1027 05:48:20.385847    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"
Oct 27 05:48:21 minikube kubelet[2378]: I1027 05:48:21.382691    2378 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="cinemaabyss/monolith-767864457d-chblg" secret="" err="secret \"dockerconfigjson\" not found"
Oct 27 05:48:29 minikube kubelet[2378]: I1027 05:48:29.383677    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:48:29 minikube kubelet[2378]: E1027 05:48:29.383978    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:48:32 minikube kubelet[2378]: E1027 05:48:32.386322    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"
Oct 27 05:48:34 minikube kubelet[2378]: E1027 05:48:34.385938    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:48:41 minikube kubelet[2378]: I1027 05:48:41.382827    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:48:41 minikube kubelet[2378]: E1027 05:48:41.383077    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:48:43 minikube kubelet[2378]: E1027 05:48:43.386053    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"
Oct 27 05:48:46 minikube kubelet[2378]: E1027 05:48:46.386125    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:48:52 minikube kubelet[2378]: I1027 05:48:52.381081    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:48:52 minikube kubelet[2378]: E1027 05:48:52.381420    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:48:56 minikube kubelet[2378]: E1027 05:48:56.361811    2378 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 27 05:48:56 minikube kubelet[2378]: E1027 05:48:56.362068    2378 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/f2425fec-9a6a-481d-bd27-84d90e0f6c0b-webhook-cert podName:f2425fec-9a6a-481d-bd27-84d90e0f6c0b nodeName:}" failed. No retries permitted until 2025-10-27 05:50:58.36202799 +0000 UTC m=+13725.522302968 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/f2425fec-9a6a-481d-bd27-84d90e0f6c0b-webhook-cert") pod "ingress-nginx-controller-77c695954b-mxl8b" (UID: "f2425fec-9a6a-481d-bd27-84d90e0f6c0b") : secret "ingress-nginx-admission" not found
Oct 27 05:48:56 minikube kubelet[2378]: I1027 05:48:56.381244    2378 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="cinemaabyss/movies-service-77cb944b49-ktxql" secret="" err="secret \"dockerconfigjson\" not found"
Oct 27 05:48:58 minikube kubelet[2378]: E1027 05:48:58.383929    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:48:58 minikube kubelet[2378]: E1027 05:48:58.383950    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"
Oct 27 05:49:04 minikube kubelet[2378]: I1027 05:49:04.380432    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:49:04 minikube kubelet[2378]: E1027 05:49:04.380750    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:49:08 minikube kubelet[2378]: E1027 05:49:08.380683    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-77c695954b-mxl8b" podUID="f2425fec-9a6a-481d-bd27-84d90e0f6c0b"
Oct 27 05:49:12 minikube kubelet[2378]: E1027 05:49:12.384009    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:49:12 minikube kubelet[2378]: E1027 05:49:12.384031    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"
Oct 27 05:49:19 minikube kubelet[2378]: I1027 05:49:19.378624    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:49:19 minikube kubelet[2378]: E1027 05:49:19.378829    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:49:23 minikube kubelet[2378]: E1027 05:49:23.389831    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:49:24 minikube kubelet[2378]: E1027 05:49:24.381003    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"
Oct 27 05:49:32 minikube kubelet[2378]: I1027 05:49:32.379610    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:49:32 minikube kubelet[2378]: E1027 05:49:32.379816    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:49:35 minikube kubelet[2378]: E1027 05:49:35.381100    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:49:39 minikube kubelet[2378]: E1027 05:49:39.383201    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"
Oct 27 05:49:43 minikube kubelet[2378]: I1027 05:49:43.379651    2378 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="cinemaabyss/monolith-767864457d-chblg" secret="" err="secret \"dockerconfigjson\" not found"
Oct 27 05:49:43 minikube kubelet[2378]: I1027 05:49:43.379754    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:49:43 minikube kubelet[2378]: E1027 05:49:43.379871    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:49:49 minikube kubelet[2378]: E1027 05:49:49.388760    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:49:50 minikube kubelet[2378]: E1027 05:49:50.380489    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"
Oct 27 05:49:56 minikube kubelet[2378]: I1027 05:49:56.376834    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:49:56 minikube kubelet[2378]: E1027 05:49:56.377109    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:50:01 minikube kubelet[2378]: E1027 05:50:01.380624    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:50:11 minikube kubelet[2378]: I1027 05:50:11.377360    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:50:11 minikube kubelet[2378]: E1027 05:50:11.377596    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ingress-controller\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ingress-controller pod=ingress-kong-58bccc89cb-xwzcm_kong(57ef0997-ebdb-44ce-9cbf-fafde53c4846)\"" pod="kong/ingress-kong-58bccc89cb-xwzcm" podUID="57ef0997-ebdb-44ce-9cbf-fafde53c4846"
Oct 27 05:50:16 minikube kubelet[2378]: E1027 05:50:16.379474    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:50:20 minikube kubelet[2378]: I1027 05:50:20.374916    2378 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="cinemaabyss/movies-service-77cb944b49-ktxql" secret="" err="secret \"dockerconfigjson\" not found"
Oct 27 05:50:24 minikube kubelet[2378]: I1027 05:50:24.375633    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:50:30 minikube kubelet[2378]: E1027 05:50:30.378288    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:50:41 minikube kubelet[2378]: E1027 05:50:41.379453    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:51:35 minikube kubelet[2378]: I1027 05:51:35.659827    2378 scope.go:117] "RemoveContainer" containerID="e9d2549295502af07dc134c2c865c83af2000f7d83361b2c937772984d0072a3"
Oct 27 05:51:37 minikube kubelet[2378]: E1027 05:51:37.170679    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-patch-b8dct" podUID="74bf0ee4-4a25-4ea1-b858-2fb757774512"
Oct 27 05:51:43 minikube kubelet[2378]: I1027 05:51:43.165694    2378 kubelet_pods.go:1082] "Unable to retrieve pull secret, the image pull may not succeed." pod="cinemaabyss/monolith-767864457d-chblg" secret="" err="secret \"dockerconfigjson\" not found"
Oct 27 05:51:43 minikube kubelet[2378]: E1027 05:51:43.237720    2378 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 27 05:51:43 minikube kubelet[2378]: E1027 05:51:43.237928    2378 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/f2425fec-9a6a-481d-bd27-84d90e0f6c0b-webhook-cert podName:f2425fec-9a6a-481d-bd27-84d90e0f6c0b nodeName:}" failed. No retries permitted until 2025-10-27 05:53:45.237898858 +0000 UTC m=+13847.613313575 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/f2425fec-9a6a-481d-bd27-84d90e0f6c0b-webhook-cert") pod "ingress-nginx-controller-77c695954b-mxl8b" (UID: "f2425fec-9a6a-481d-bd27-84d90e0f6c0b") : secret "ingress-nginx-admission" not found
Oct 27 05:51:59 minikube kubelet[2378]: E1027 05:51:59.443015    2378 log.go:32] "PullImage from image service failed" err="rpc error: code = Canceled desc = context canceled" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Oct 27 05:51:59 minikube kubelet[2378]: E1027 05:51:59.443150    2378 kuberuntime_image.go:43] "Failed to pull image" err="rpc error: code = Canceled desc = context canceled" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Oct 27 05:51:59 minikube kubelet[2378]: E1027 05:51:59.443548    2378 kuberuntime_manager.go:1449] "Unhandled Error" err="container create start failed in pod ingress-nginx-admission-create-hvvg7_ingress-nginx(38200aff-98ca-49b4-9548-f28f9140bb22): ErrImagePull: rpc error: code = Canceled desc = context canceled" logger="UnhandledError"
Oct 27 05:51:59 minikube kubelet[2378]: E1027 05:51:59.443610    2378 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ErrImagePull: \"rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-hvvg7" podUID="38200aff-98ca-49b4-9548-f28f9140bb22"


==> storage-provisioner [a4206373e0df] <==
W1027 05:50:15.611442       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:15.618656       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:17.620939       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:17.628407       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:19.632736       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:19.641884       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:21.647366       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:21.655948       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:23.660405       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:23.666645       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:25.672846       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:25.681195       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:27.685146       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:27.692087       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:29.696096       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:29.702282       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:31.708109       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:31.719981       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:33.726926       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:33.737012       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:35.741280       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:35.751234       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:37.755845       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:37.762609       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:39.766639       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:39.775131       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:41.780371       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:41.787848       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:43.793005       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:43.799403       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:45.803755       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:50:45.809816       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:32.610818       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:32.624768       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:34.637354       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:34.666466       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:36.673162       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:36.684804       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:38.690788       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:38.707480       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:40.712022       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:40.717777       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:42.722624       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:42.730788       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:44.736314       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:44.745505       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:46.750672       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:46.756853       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:48.761206       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:48.766771       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:50.770140       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:50.777814       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:52.782087       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:52.789475       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:54.793686       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:54.799162       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:56.804068       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:56.810292       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:58.814422       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1027 05:51:58.823639       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

